{
    "version": "https://jsonfeed.org/version/1",
    "title": "雨伞的个人博客 • All posts by \"一\" tag",
    "description": "",
    "home_page_url": "http://example.com",
    "items": [
        {
            "id": "http://example.com/eca31b5db33e/",
            "url": "http://example.com/eca31b5db33e/",
            "title": "scrapy",
            "date_published": "2023-11-08T11:50:03.000Z",
            "content_html": "<figure class=\"highlight clean\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br><span class=\"line\">72</span><br><span class=\"line\">73</span><br><span class=\"line\">74</span><br><span class=\"line\">75</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs clean\"><span class=\"hljs-keyword\">import</span> datetime<br><span class=\"hljs-keyword\">import</span> json<br><span class=\"hljs-keyword\">from</span> pprint <span class=\"hljs-keyword\">import</span> pprint<br><br><span class=\"hljs-keyword\">import</span> requests<br><br>headers = &#123;  # 请求头。<br>    # 假装请求是浏览器发出的（在这里实际上是网络爬虫发出的）。<br>    # 有些网站会通过检测请求头的User-Agent字段判断请求是<br>    # 浏览器发出的还是爬虫发出的。如果检测出是爬虫发出的，<br>    # 则拒绝相应。<br>    <span class=\"hljs-string\">&#x27;User-Agent&#x27;</span>: <span class=\"hljs-string\">&#x27;Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/98.0.4758.80 Safari/537.36&#x27;</span><br>&#125;<br><br>invalid_chars_for_windows_filenames = [<span class=\"hljs-string\">&#x27;/&#x27;</span>, <span class=\"hljs-string\">&#x27;\\\\&#x27;</span>, <span class=\"hljs-string\">&#x27;:&#x27;</span>, <span class=\"hljs-string\">&#x27;*&#x27;</span>, <span class=\"hljs-string\">&#x27;?&#x27;</span>, <span class=\"hljs-string\">&#x27;&quot;&#x27;</span>, <span class=\"hljs-string\">&#x27;&lt;&#x27;</span>, <span class=\"hljs-string\">&#x27;&gt;&#x27;</span>, <span class=\"hljs-string\">&#x27;|&#x27;</span>]<br><br><br>def fix_filename(filename):<br>    for char <span class=\"hljs-keyword\">in</span> invalid_chars_for_windows_filenames:<br>        filename = filename.replace(char, <span class=\"hljs-string\">&quot;&quot;</span>)<br>    return filename<br><br><br>def main():<br>    rcmd_api = <span class=\"hljs-string\">&quot;https://api.bilibili.com/x/web-interface/wbi/index/top/feed/rcmd&quot;</span><br>    # rcmd_api = <span class=\"hljs-string\">&quot;https://api.bilibili.com/x/web-interface/wbi/index/top/feed/rcmd?ps=30&quot;</span><br><br>    # 通过api请求json数据。<br>    r = requests.get(<br>        url=rcmd_api,<br>        headers=headers,<br>    )<br>    json_str = r.text  # Not html.<br><br>    # 将请求得到的json字符串转换成Python字典。<br>    d = json.loads(json_str)<br>    pprint(d)<br><br>    items = d[<span class=\"hljs-string\">&quot;data&quot;</span>][<span class=\"hljs-string\">&quot;item&quot;</span>]<br><br>    rcmd_videos = []<br>    for item <span class=\"hljs-keyword\">in</span> items:<br>        # 获取封面图。<br>        pic = item[<span class=\"hljs-string\">&quot;pic&quot;</span>]<br><br>        # 获取标题。<br>        title = item[<span class=\"hljs-string\">&quot;title&quot;</span>]<br><br>        # 获取发布日期。<br>        pubdate = item[<span class=\"hljs-string\">&quot;pubdate&quot;</span>]<br>        pubdate = datetime.datetime.fromtimestamp(pubdate)<br><br>        # 获取up主。<br>        owner_name = item[<span class=\"hljs-string\">&quot;owner&quot;</span>][<span class=\"hljs-string\">&quot;name&quot;</span>]<br><br>        rcmd_videos.append(&#123;<br>            <span class=\"hljs-string\">&quot;pic&quot;</span>: pic,<br>            <span class=\"hljs-string\">&quot;title&quot;</span>: title,<br>            <span class=\"hljs-string\">&quot;pubdate&quot;</span>: pubdate,<br>            <span class=\"hljs-string\">&quot;owner_name&quot;</span>: owner_name,<br>        &#125;)<br><br>        # Save the image.<br>        # r = requests.get(<br>        #     url=pic,<br>        #     headers=headers<br>        # )<br>        # image = r.content  # Not .text.<br>        # <span class=\"hljs-keyword\">with</span> open(f<span class=\"hljs-string\">&quot;&#123;fix_filename(title)&#125;.jpg&quot;</span>, <span class=\"hljs-string\">&quot;wb&quot;</span>) <span class=\"hljs-keyword\">as</span> f:<br>        #     f.write(image)<br><br><br><span class=\"hljs-keyword\">if</span> __name__ == <span class=\"hljs-string\">&#x27;__main__&#x27;</span>:<br>    main()<br>    <br></code></pre></td></tr></table></figure>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br><span class=\"line\">72</span><br><span class=\"line\">73</span><br><span class=\"line\">74</span><br><span class=\"line\">75</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs python\"><span class=\"hljs-keyword\">from</span> pprint <span class=\"hljs-keyword\">import</span> pprint<br><br><span class=\"hljs-keyword\">import</span> requests<br><span class=\"hljs-keyword\">from</span> bs4 <span class=\"hljs-keyword\">import</span> BeautifulSoup<br><br><br><span class=\"hljs-keyword\">def</span> <span class=\"hljs-title function_\">obtain_channel_info</span>(<span class=\"hljs-params\">soup</span>):<br>    channels = []  <span class=\"hljs-comment\"># 存储各分区信息的列表。</span><br><br>    <span class=\"hljs-comment\"># 获取包含各分区a标签的div标签。</span><br>    channel_div = soup.find(<br>        name=<span class=\"hljs-string\">&quot;div&quot;</span>,<br>        class_=<span class=\"hljs-string\">&quot;channel-items__left&quot;</span>,<br>    )<br>    <span class=\"hljs-comment\"># print(channel_div.prettify())</span><br><br>    <span class=\"hljs-comment\"># 获取各个a标签。</span><br>    channel_as = channel_div.find_all(<br>        name=<span class=\"hljs-string\">&quot;a&quot;</span>,<br>    )<br><br>    <span class=\"hljs-comment\"># 从各个a标签获取分区名和分区链接。</span><br>    <span class=\"hljs-keyword\">for</span> channel_a <span class=\"hljs-keyword\">in</span> channel_as:<br>        <span class=\"hljs-comment\"># 获取分区名。</span><br>        channel_name = channel_a.text<br><br>        <span class=\"hljs-comment\"># 获取分区链接。</span><br>        channel_href = channel_a[<span class=\"hljs-string\">&quot;href&quot;</span>]<br>        channel_href = <span class=\"hljs-string\">&quot;https:&quot;</span> + channel_href<br>        <span class=\"hljs-comment\"># channel_href = f&quot;https:&#123;channel_href&#125;&quot;</span><br><br>        <span class=\"hljs-comment\"># 将分区名和分区链接装进字典，将该字典加入channels列表。</span><br>        channels.append(&#123;<br>            <span class=\"hljs-string\">&quot;name&quot;</span>: channel_name,<br>            <span class=\"hljs-string\">&quot;href&quot;</span>: channel_href,<br>        &#125;)<br><br>    pprint(channels)<br><br>    <span class=\"hljs-comment\"># 保存分区信息。</span><br>    <span class=\"hljs-keyword\">with</span> <span class=\"hljs-built_in\">open</span>(<span class=\"hljs-string\">&quot;channels.txt&quot;</span>, <span class=\"hljs-string\">&quot;w&quot;</span>, encoding=<span class=\"hljs-string\">&quot;utf-8&quot;</span>) <span class=\"hljs-keyword\">as</span> f:<br>        f.write(<span class=\"hljs-string\">&quot;name,href\\n&quot;</span>)<br>        <span class=\"hljs-keyword\">for</span> channel <span class=\"hljs-keyword\">in</span> channels:<br>            f.write(<span class=\"hljs-string\">f&quot;<span class=\"hljs-subst\">&#123;channel[<span class=\"hljs-string\">&#x27;name&#x27;</span>]&#125;</span>,<span class=\"hljs-subst\">&#123;channel[<span class=\"hljs-string\">&#x27;href&#x27;</span>]&#125;</span>\\n&quot;</span>)<br><br><br><span class=\"hljs-keyword\">def</span> <span class=\"hljs-title function_\">main</span>():<br>    url = <span class=\"hljs-string\">&quot;https://www.bilibili.com/&quot;</span>  <span class=\"hljs-comment\"># B站URL。</span><br>    headers = &#123;  <span class=\"hljs-comment\"># 请求头。</span><br>        <span class=\"hljs-comment\"># 假装请求是浏览器发出的（在这里实际上是网络爬虫发出的）。</span><br>        <span class=\"hljs-comment\"># 有些网站会通过检测请求头的User-Agent字段判断请求是</span><br>        <span class=\"hljs-comment\"># 浏览器发出的还是爬虫发出的。如果检测出是爬虫发出的，</span><br>        <span class=\"hljs-comment\"># 则拒绝相应。</span><br>        <span class=\"hljs-string\">&#x27;User-Agent&#x27;</span>: <span class=\"hljs-string\">&#x27;Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/98.0.4758.80 Safari/537.36&#x27;</span><br>    &#125;<br><br>    <span class=\"hljs-comment\"># 第一步：获取html。</span><br>    r = requests.get(  <span class=\"hljs-comment\"># 向B站发出请求，并获取B站的相应。</span><br>        url=url,  <span class=\"hljs-comment\"># 指定URL。</span><br>        headers=headers,  <span class=\"hljs-comment\"># 指定请求头。</span><br>    )<br>    html = r.text  <span class=\"hljs-comment\"># 从B站响应（变量r）中获取html。</span><br><br>    <span class=\"hljs-comment\"># 第二步：解析html。</span><br>    soup = BeautifulSoup(<br>        markup=html,  <span class=\"hljs-comment\"># 指定html。</span><br>        features=<span class=\"hljs-string\">&quot;html.parser&quot;</span>,  <span class=\"hljs-comment\"># 指定解析器。</span><br>    )<br><br>    <span class=\"hljs-comment\"># 第三步：获取分区信息（分区名和分区链接）。</span><br>    obtain_channel_info(soup=soup)<br><br><br><span class=\"hljs-keyword\">if</span> __name__ == <span class=\"hljs-string\">&#x27;__main__&#x27;</span>:<br>    main()<br></code></pre></td></tr></table></figure>\n",
            "tags": [
                "一"
            ]
        }
    ]
}